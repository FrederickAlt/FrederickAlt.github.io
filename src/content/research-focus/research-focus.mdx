---

---

My current research and PHD thesis focuses on understanding the training dynamics of ultra wide neural networks using **probability theory**. In particular, I study the emergence of the **Neural Tangent Kernel (NTK)** regime and the $\mu P$ regime as characterized by Grag Yang. I show that even in rich settings like the $\mu P$ regime the layers doucple stochastically for all training time. Furthermore i draw parallels to **lazey** and **rich** training regimes and the phenomenon of **Grokking**.


